# HW5 - Vagrant стенд для NFS

> Научиться самостоятельно развернуть сервис NFS и подключить к нему клиента

# 1. Создаём тестовые виртуальные машины

> Для начала, предлагается использовать этот шаблон для создания виртуальных машин
```
# -*- mode: ruby -*-
# vi: set ft=ruby :
Vagrant.configure(2) do |config|
  config.vm.box = "centos/7"
  config.vm.box_version = "2004.01"

  config.vm.provider "virtualbox" do |v|
    v.memory = 1024
    v.cpus = 1
  end

  config.vm.define "nfss" do |nfss|
    nfss.vm.network "private_network", ip: "192.168.50.10", virtualbox__intnet: "net1"
    nfss.vm.hostname = "nfss"
  end

  config.vm.define "nfsc" do |nfsc|
    nfsc.vm.network "private_network", ip: "192.168.50.11", virtualbox__intnet: "net1"
    nfsc.vm.hostname = "nfsc"
  end
end
```

# 2. Настраиваем сервер NFS
```
vagrant ssh nfss
```
> Дальнейшие действия выполняются от имени пользователя имеющего повышенные привилегии, разрешающие описанные действия.
> сервер NFS уже установлен в CentOS 7 как часть дистрибутива, так что нам нужно лишь доустановить утилиты, которые облегчат отладку

```
sudo -i
yum install nfs-utils -y
```

> включаем firewall и проверяем, что он работает
> (доступ к SSH обычно включен по умолчанию, поэтому здесь мы его не затрагиваем, но имем это ввиду, если настраиваем firewall с нуля)

```
systemctl enable firewalld --now
systemctl status firewalld
```

> разрешаем в firewall доступ к сервисам NFS 
```
firewall-cmd --add-service="nfs3" \
--add-service="rpc-bind" \
--add-service="mountd" \
--permanent
firewall-cmd --reload
```
> включаем сервер NFS
> для конфигурации NFSv3 over UDP он не требует дополнительной настройки, однако мы можем ознакомиться с умолчаниями в файле /etc/nfs.conf

```
systemctl enable nfs --now
```

> проверяем наличие слушаемых портов 2049/udp, 2049/tcp, 20048/udp, 20048/tcp, 111/udp, 111/tcp
```
ss -tnplu | grep -E '2049|20048|111'
udp    UNCONN     0      0         *:20048                 *:*                   users:(("rpc.mountd",pid=2294,fd=7))
udp    UNCONN     0      0         *:111                   *:*                   users:(("rpcbind",pid=331,fd=6))
udp    UNCONN     0      0         *:2049                  *:*                  
udp    UNCONN     0      0      [::]:20048              [::]:*                   users:(("rpc.mountd",pid=2294,fd=9))
udp    UNCONN     0      0      [::]:111                [::]:*                   users:(("rpcbind",pid=331,fd=9))
udp    UNCONN     0      0      [::]:2049               [::]:*                  
tcp    LISTEN     0      128       *:111                   *:*                   users:(("rpcbind",pid=331,fd=8))
tcp    LISTEN     0      128       *:20048                 *:*                   users:(("rpc.mountd",pid=2294,fd=8))
tcp    LISTEN     0      64        *:2049                  *:*                  
tcp    LISTEN     0      128    [::]:111                [::]:*                   users:(("rpcbind",pid=331,fd=11))
tcp    LISTEN     0      128    [::]:20048              [::]:*                   users:(("rpc.mountd",pid=2294,fd=10))
tcp    LISTEN     0      64     [::]:2049               [::]:*                  
```

> создаём и настраиваем директорию, которая будет экспортирована в будущем

```
mkdir -p /srv/share/upload
chown -R nfsnobody:nfsnobody /srv/share
chmod 0777 /srv/share/upload
```

> создаём в файле /etc/exports структуру, которая позволит экспортировать ранее созданную директорию

```
cat << EOF > /etc/exports
/srv/share 192.168.50.11/32(rw,sync,root_squash)
EOF
```

> экспортируем ранее созданную директорию

```
exportfs -r
```

> проверяем экспортированную директорию следующей командой

```
exportfs -s
/srv/share  192.168.50.11/32(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,root_squash,no_all_squash)
```

# 3. Настраиваем клиент NFS

> заходим на клиент

```
vagrant ssh nfsc
```

> доустановим вспомогательные утилиты

```
yum install nfs-utils -y
```

> включаем firewall и проверяем, что он работает

```
systemctl enable firewalld --now
systemctl status firewalld
```

> добавляем в /etc/fstab строку

```
echo "192.168.50.10:/srv/share/ /mnt nfs vers=3,proto=udp,noauto,x-systemd.automount 0 0" >> /etc/fstab
```

> и выполняем

> Отметим, что в данном случае происходит автоматическая генерация systemd units в каталоге /run/systemd/generator/,
> которые производят монтирование при первом обращении к каталогу /mnt/


```
cat /run/systemd/generator/mnt.mount 
# Automatically generated by systemd-fstab-generator

[Unit]
SourcePath=/etc/fstab
Documentation=man:fstab(5) man:systemd-fstab-generator(8)

[Mount]
What=192.168.50.10:/srv/share/
Where=/mnt
Type=nfs
Options=vers=3,proto=udp,noauto,xsystemd.automount
```
> заходим в директорию /mnt/ и проверяем успешность монтирования

```
ls /mnt
mount | grep mnt
```

> При успехе вывод должен примерно соответствовать этому

```
mount | grep mnt
systemd-1 on /mnt type autofs (rw,relatime,fd=25,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=20327)
192.168.50.10:/srv/share/ on /mnt type nfs (rw,relatime,vers=3,rsize=32768,wsize=32768,namlen=255,hard,proto=udp,timeo=11,retrans=3,sec=sys,mountaddr=192.168.50.10,mountvers=3,mountport=20048,mountproto=udp,local_lock=none,addr=192.168.50.10)
```

# 4. Проверка работоспособности

> заходим на сервер
> заходим в каталог

```
cd /srv/share/upload
```

> создаём тестовый файл

```
touch check_file
```

> заходим на клиент
> заходим в каталог

```
cd /mnt/upload
```

> проверяем наличие ранее созданного файла

```
ls
check_file
```

> создаём тестовый файл

```
touch client_file
```

> проверяем, что файл успешно создан и доступен на сервере

```
ls
check_file  client_file
```

> Предварительно проверяем клиент:
> перезагружаем клиент
> заходим на клиент
> заходим в каталог

```
cd /mnt/upload
```

> проверяем наличие ранее созданных файлов

```
ls
check_file  client_file
```

> Проверяем сервер:
> заходим на сервер в отдельном окне терминала
> перезагружаем сервер
> проверяем наличие файлов в каталоге

```
cd /srv/share/upload/
ls
check_file  client_file
```

> проверяем статус сервера NFS
 
```
systemctl status nfs
● nfs-server.service - NFS server and services
   Loaded: loaded (/usr/lib/systemd/system/nfs-server.service; enabled; vendor preset: disabled)
   Active: active (exited) since Thu 2023-06-01 10:41:24 UTC; 1h 4min ago
   Process: 3609 ExecStartPost=/bin/sh -c if systemctl -q is-active gssproxy; then systemctl reload gssproxy ; fi (code=exited, status=0/SUCCESS)
   Process: 3592 ExecStart=/usr/sbin/rpc.nfsd $RPCNFSDARGS (code=exited, status=0/SUCCESS)
   Process: 3591 ExecStartPre=/usr/sbin/exportfs -r (code=exited, status=0/SUCCESS)
   Main PID: 3592 (code=exited, status=0/SUCCESS)
   CGroup: /system.slice/nfs-server.service

Jun 01 10:41:24 nfss systemd[1]: Starting NFS server ...
Jun 01 10:41:24 nfss systemd[1]: Started NFS server a...
Hint: Some lines were ellipsized, use -l to show in full.
```

> проверяем статус firewall 

```
● firewalld.service - firewalld - dynamic firewall daemon
   Loaded: loaded (/usr/lib/systemd/system/firewalld.service; enabled; vendor preset: enabled)
   Active: active (running) since Thu 2023-06-01 11:10:52 UTC; 35min ago
     Docs: man:firewalld(1)
 Main PID: 2134 (firewalld)
   CGroup: /system.slice/firewalld.service
           └─2134 /usr/bin/python2 -Es /usr/sbin/firewalld --nofork --nopid

Jun 01 11:10:52 nfss systemd[1]: Starting firewalld -...
Jun 01 11:10:52 nfss systemd[1]: Started firewalld - ...
Jun 01 11:10:52 nfss firewalld[2134]: WARNING: Allow...
Hint: Some lines were ellipsized, use -l to show in full.
```

> проверяем экспорты

```
exportfs -s
/srv/share  192.168.50.11/32(sync,wdelay,hide,no_subtree_check,sec=sys,rw,secure,root_squash,no_all_squash)
```

> проверяем работу RPC

```
showmount -a 192.168.50.10

All mount points on 192.168.50.10:
192.168.50.11:/srv/share
```

> Проверяем клиент:
> возвращаемся на клиент
> перезагружаем клиент
> заходим на клиент
> проверяем работу RPC 

```
showmount -a 192.168.50.10
All mount points on 192.168.50.10:
192.168.50.11:/srv/share
```

> заходим в каталог

```
cd /mnt/upload
```
> проверяем статус монтирования
```
mount | grep mnt
systemd-1 on /mnt type autofs (rw,relatime,fd=33,pgrp=1,timeout=0,minproto=5,maxproto=5,direct,pipe_ino=10916)
192.168.50.10:/srv/share/ on /mnt type nfs (rw,relatime,vers=3,rsize=32768,wsize=32768,namlen=255,hard,proto=udp,timeo=11,retrans=3,sec=sys,mountaddr=192.168.50.10,mountvers=3,mountport=20048,mountproto=udp,local_lock=none,addr=192.168.50.10)
```

> проверяем наличие ранее созданных файлов
> создаём тестовыйфайл

```
touch final_check
```
> проверяем, что файл успешно создан

```
check_file  client_file  final_check
```


# 5. Создание автоматизированного Vagrantfile

> Ранее предложенный Vagrantfile предлагается дополнить до такого
```
# -*- mode: ruby -*-
# vi: set ft=ruby :
Vagrant.configure(2) do |config|
  config.vm.box = "centos/7"
  config.vm.box_version = "2004.01"

  config.vm.provider "virtualbox" do |v|
    v.memory = 1024
   v.cpus = 1
  end

  config.vm.define "nfss" do |nfss|
    nfss.vm.network "private_network", ip: "192.168.50.10", virtualbox__intnet: "net1"
    nfss.vm.hostname = "nfss"
    nfss.vm.provision "shell", path: "nfss_script.sh"
  end

  config.vm.define "nfsc" do |nfsc|
    nfsc.vm.network "private_network", ip: "192.168.50.11", virtualbox__intnet: "net1"
    nfsc.vm.hostname = "nfsc"
    nfsc.vm.provision "shell", path: "nfsc_script.sh"
  end
end
```

# Далее создадим 2 bash-скрипта, nfss_script.sh - для конфигурирования сервера и nfsc_script.sh - для конфигурирования клиента, в которых опищем bash-командами ранее выполненные шаги

> nfss_script.sh
```
#!/bin/bash

yum install nfs-utils -y

systemctl enable firewalld --now
systemctl status firewalld

firewall-cmd --add-service="nfs3" \
--add-service="rpc-bind" \
--add-service="mountd" \
--permanent
firewall-cmd --reload

systemctl enable nfs --now

mkdir -p /srv/share/upload
chown -R nfsnobody:nfsnobody /srv/share
chmod 0777 /srv/share/upload

cat << EOF > /etc/exports
/srv/share 192.168.50.11/32(rw,sync,root_squash)
EOF

exportfs -r
```

> nfsc_script.sh

```
yum install nfs-utils -y

systemctl enable firewalld --now
systemctl status firewalld

echo "192.168.50.10:/srv/share/ /mnt nfs vers=3,proto=udp,noauto,x-systemd.automount 0 0" >> /etc/fstab

systemctl daemon-reload
systemctl restart remote-fs.target
```

# Теперь уничтожим тестовый стенд командой vagrant destory -f, создадим его заново и выполним все пункты из 5 - проверка работоспособности, убедимся, что всё работает как задумывалось и требуется
